# App/models/model_loader.py

# from pathlib import Path
# import joblib
# import torch
# import numpy as np
# from . import md_inference
# # md_inference ìª½ì— ì´ë¯¸ ì •ì˜ëœ ì•„í‚¤í…ì²˜/ê²½ë¡œ/ë””ë°”ì´ìŠ¤ ì¬ì‚¬ìš©
# from .md_inference import (
#     DEVICE,
#     IDLE_MODEL_PATH,
#     IDLE_IMPORTANCE_MASK_PATH,
#     IDLE_IDX_TO_LABEL,
#    EnsembleVoteModel,
#     StartupEnsemble,
#     STARTUP_MODEL_PATH,
#     STARTUP_LABEL_PATH,
#     # ğŸ’¡ md_inference.pyì˜ ë³€ìˆ˜ ì´ë¦„ì„ ì‚¬ìš©
#     BRAKE_INPUT_MEL_SHAPE,
#     BRAKE_INPUT_FFT_SIZE,
# )


# from .idle_ensemble import WaveformCNN1D, MaskedCNN, EnsembleVoteModel
# from .brake_hybrid import Hybrid_Model

# # ì´ íŒŒì¼ì´ ìˆëŠ” í´ë”: App/models
# MODEL_DIR = Path(__file__).resolve().parent

# # ----- braking (hybrid) ê²½ë¡œ -----
# BRAKE_MODEL_PATH = MODEL_DIR /"brakes.pth"


# # ==========================================================
# # 1) braking í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë” (Hybrid_Model)
# # ==========================================================
# # ==========================================================
# # 1) braking í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë” (Hybrid_Model)
# # ==========================================================


# def _load_braking_model(device: str = "cpu"):
#     device = torch.device(device)
#     model = Hybrid_Model(
#         mel_shape=BRAKE_INPUT_MEL_SHAPE,
#         fft_size=BRAKE_INPUT_FFT_SIZE,
#     ).to(device)

#     state_dict = torch.load(BRAKE_MODEL_PATH, map_location=device)
#     model.load_state_dict(state_dict)
#     model.eval()
#     print(f"[MD] braking model loaded from {BRAKE_MODEL_PATH}")
#     return model

# def load_md_models():
#     models = {}
#     models["braking"] = _load_braking_model(device=DEVICE)
#     print("MD_MODELS loaded:", {k: type(v) for k, v in models.items()})
#     return models
# # ------------------------------------------------
# # 1) idle ì•™ìƒë¸” ëª¨ë¸ ë¡œë” (PyTorch)
# # ------------------------------------------------
# def _load_idle_model(device: str = "cpu"):
#     """
#     idle.pth ì— ì €ì¥ëœ state_dictë¥¼ ì´ìš©í•´
#     md_inferenceì™€ ë™ì¼í•œ EnsembleVoteModel êµ¬ì¡°ì— ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•œë‹¤.
#     brakingì€ ì—¬ê¸°ì„œ ë‹¤ë£¨ì§€ ì•ŠìŒ (md_inference ë‚´ë¶€ì—ì„œ ì²˜ë¦¬).
#     """
#     # Flask ì„œë²„ì—ì„œ CPUë§Œ ì“¸ ê±°ë¼ë©´ DEVICE ëŒ€ì‹  ìƒˆë¡œ ì¡ì•„ë„ ë¨
#     device = torch.device(device)

#     num_classes = len(IDLE_IDX_TO_LABEL)  # idle í´ë˜ìŠ¤ ê°œìˆ˜ :contentReference[oaicite:3]{index=3}

#     # ì¤‘ìš”ë„ ë§ˆìŠ¤í¬ ë¡œë“œ (ìˆìœ¼ë©´)
#     importance_mask = None
#     if IDLE_IMPORTANCE_MASK_PATH.exists():
#         mask_np = np.load(IDLE_IMPORTANCE_MASK_PATH)
#         # md_inference._ensure_idle_model ê³¼ ë™ì¼í•œ í˜•íƒœ (1, 128, 216) ì°¸ê³  :contentReference[oaicite:4]{index=4}
#         importance_mask = torch.FloatTensor(mask_np).unsqueeze(0).to(device)

#     # md_inferenceì™€ ê°™ì€ êµ¬ì¡°ì˜ ì•™ìƒë¸” ëª¨ë¸ ìƒì„± :contentReference[oaicite:5]{index=5}
#     model = EnsembleVoteModel(
#         num_classes=num_classes,
#         base_channels_wf=32,
#         base_channels_mel=64,
#         base_channels_mfcc=64,
#     ).to(device)

#     state_dict = torch.load(IDLE_MODEL_PATH, map_location=device)
#     model.load_state_dict(state_dict)
#     model.eval()

#     return model, importance_mask


# # ------------------------------------------------
# # 2) startup ì•™ìƒë¸” ëª¨ë¸ ë¡œë” (PyTorch)
# # ------------------------------------------------
# def _load_startup_model(device: str = "cpu"):
#     """
#     startup.pth + startup_label_encoder.pkl ë¡œë¶€í„°
#     StartupEnsemble ëª¨ë¸ê³¼ LabelEncoder ë¥¼ ë¡œë“œí•œë‹¤.
#     """
#     device = torch.device(device)

#     # LabelEncoder ë¡œë“œ :contentReference[oaicite:6]{index=6}
#     encoder = joblib.load(STARTUP_LABEL_PATH)
#     num_classes = len(encoder.classes_)

#     # md_inference ì™€ ë™ì¼í•œ StartupEnsemble êµ¬ì¡° ì‚¬ìš© :contentReference[oaicite:7]{index=7}
#     model = StartupEnsemble(num_classes=num_classes).to(device)

#     state_dict = torch.load(STARTUP_MODEL_PATH, map_location=device)
#     model.load_state_dict(state_dict)
#     model.eval()

#     return model, encoder


# # ==========================================================
# # 4) Flask ì—ì„œ ì‚¬ìš©í•  ìµœì¢… dict ìƒì„±
# # ==========================================================
from pathlib import Path

MODEL_DIR = Path(__file__).resolve().parent

def load_md_models():
    """
    MD ì „ìš© ëª¨ë¸ì€ md_inference.py ì—ì„œ lazy-load ë˜ê¸° ë•Œë¬¸ì—
    ì—¬ê¸°ì„œëŠ” í˜•ì‹ ë§ì¶”ê¸°ìš© ë¹ˆ dictë§Œ ë°˜í™˜í•œë‹¤.
    """
    models = {}
    print("MD_MODELS loaded (placeholder):", models)
    return models